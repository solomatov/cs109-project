<!doctype html>
<html>
<head>
  <title>Can you trust doctor Robot?</title>

  <link rel="stylesheet" href="lib/jquery.tocify.css" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>
  <script src="//code.jquery.com/jquery-1.11.3.min.js"></script>
  <script src="https://code.jquery.com/ui/1.11.3/jquery-ui.min.js"></script>
  <script src="lib/jquery.tocify.min.js"></script>

  <style type="text/css">
    .container {
      padding-top: 20px;
      padding-left: 250px;
    }

    #toc {
      margin-top: 20px;
    }
  </style>

</head>
<body>
  <div id="toc">
  </div>


<div class="container">
clean  <div class="jumbotron">
    <h1>Can you trust doctor Robot?</h1>
    <p>
      It's late in the week and work is piling up. You run down the escalator to catch the northbound
      tube. Anchovised with your fellow commuters you catch your breath when you feel a sudden chest
      pain. At the same time you get nauesous and your vision just blurs until you pass out.
    </p>
    <p>
      You wake up just as the ambulance enters Central ER, feeling a little dizzy you still have all your
      senses. At arrival you are welcomed by a shiny new doctor robot from Healthy Hearts Tech. A paramedic
      takes a brief look at you and adjusts a bar on the robots touchscreen brightly labeled ROC
      sensitivity before running to the next patient. As the robots commences it's examination of you
      the paramedic leaves and you get some final advice: "If it starts to stutter just press the big
      red button.
    </p>
  </div>


  <h1 id="Intro">Project Introduction</h1>
  <h2>Brief</h2>
  <p>
    When patient arrives into a hospital's ER, medical personnel has important decisions to make. Some
    patients should be discharged immediately, some could wait, and some should be attended by a doctor
    immediately and then be hospitalized. This paper investigates if Machine Learning methods can be used as tool
	helping the ER departments optimizing their logistics by giving an early indication if  patient needs to be
	hospitalized.
  </p>
  <h2>About the ER</h2>
  <p>ER PRIORITIES<br>
    Here's a sample dashboard that is displayed on a large wall screen in the ER department. The dashboard
    shows the status of ER from a patient logistics perspective.
  </p>
  <img src="images/screen.png" width="900px" />
  <p>
    Key performance measures are tracked in the above dashboard are:
  </p>
  <ul>
    <li>Waiting for triage</li>
    <li>Present at ER
      <ul>
        <li>Present: Waited more than 8h</li>
        <li>Placement decided but waiting for hospital bed</li>
      </ul>
    </li>
  </ul>
  <p>
	ER TIMELINE<br>
    The following picture shows the typical timeline for inpatient and other patients:
  </p>
  <img src="images/er-timeline.png" />
	The hospital directors are putting pressure on the ER to shorten the length of stay and ER says
	that the main reason for long length of stay is that the patient has to wait for a hospital bed
	in the ward where the patients has been placed.


  <h2>Our Goals</h2>
  <p>
    We want to use machine learning methods to predict as early as possible whether a person coming into ER
    will require hospitalization. There's a trade off here. On one hand, we want to minimize a number of hospitalized
    patients, hospitalization is very expensive and saving money will let the hospital to provide care in more
    important areas. On the other hand, we don't want to send home a patient who will have a complication, or worse
    die because of this. Human life and health are priceless but with limited resources hard decisions need to be taken
    and predictive analysis can help with those decisions.
  </p>

  <h1 id="Method">Method</h1>
  <h2>Data Gathering</h2>
  <p>
    Our data set is the ER log. In the ER log patients are tracked from when
    they enter the ER until they exit the ER. The data in the ER log have been extracted from the hospital's
    main system for  keeping electronic medical records and covers the majority of the ER wards in the
    reception area.
  </p>
  <h2>EDA - analyzing the data and it's features</h2>
  <p>
    The Er Log the project has access to has 100+ fields and one defensive initial idea are to focus on the triage
    data. The first examination plus the medical history of the patient are what nurses and doctor are using to assess
    the status of a patient and also to give a patient an initial priority. Many Swedish hospital is using Retts
    (<a href="http://predicare.se/en/om-retts/">http://predicare.se/en/om-retts/</a>) decision support system for
    emergency medicine when assessing the patients and at the time of assessment a number of vital parameters are
    collected.
  </p>
  <p>
    In that way the project piggy rides on the domain knowledge of the organisation and try to use features that they
    use. We will see if that is a good or bad idea, but we can always extend with more features later on. We only have
    access to the ER log and not the medical history of the patients and there already from the beginning we have some
    limitations that most probably will affect the precision of our predictions. In addition to that we do not get the
    “touch and feel” a nurse and doctor get’s when talking to a patient, they can use their hands to feel body
    temperature, sweating and pinpoint things like abdominal pains etc.
  </p>
  <p>The vital parameters that we have access to are:</p>
  <ul>
    <li>Breathing frequency</li>
    <li>Saturation Status</li>
    <li>Body temperature</li>
    <li>Bllodpressure Systolic Upper</li>
    <li>Blood pressure Systolic Lower</li>
    <li>GCS Eye opening Status</li>
    <li>GCS Motoric Status</li>
    <li>GCS Verbal Status</li>
    <li>Stridor Status</li>
    <li>Pulse frequency regular</li>
    <li>Pulse frequency irregular</li>
    <li>Consciousness Retts scale status</li>
    <li>Saturation with oxygen status</li>
  </ul>
  <p>
    In addition to that we will use patient Age. It seems that ER visits highly depend on the month, day of week,
    and hour of day, so we will add these features as well.
  </p>
  <p>
    We aim to predict if a patient will need further care and become an Inpatient. We have two categories: Others and
    Inpatients. Others include people going home and outpatients. The main difference between an outpatient and an
    inpatient from a hospital logistics perspective is that the inpatients stays overnight and requires more resources.
    An outpatient may have an appointment at the hospital for example the following day or a week later for check up but
    their direct interaction (and use of resources) with the hospital is relatively brief.
  </p>
  <p>
    On this chart we display a
    bar chart of number of inpatients vs others by year:
  </p>
  <img src="images/inpatient-others.png" />
  <p>
    As we see, there're much more Others than Inpatients  which gives us an unbalanced data set with about four Other
    patienets for every Inpatient.
  </p>
  <p>
    On this chart we display histograms of different variables by inpatient/other status:
  </p>
  <img src="images/dists.png" />
  <p>
    As we see, there's a difference in distribution in almost all variables. This gives us hope that we can
    predict whether the patient will be inpatient or outpatient.
  </p>
  <p>
    As input variables we use aforementioned vital parameters, patient age, and date information. We separated data
    into training and tests set with 20% data in the test set and used test set only for final evaluation so that
    the data won't leak into the models. As a metric we use AUC on a ROC curve.
  </p>

  <h1 id="Analysis">Analysis</h1>
  <p>
    As a baseline, we will use the model which sends all patients home. It's correct in about 80.1% of the cases.
    This model is very imprecise, and doesn't take into account the cost of lost lives and health, as well as cost
    of the treatment, but it's ok as a baseline. In order to
  </p>
  <h2>Feature Evaluation</h2>
  <p>
    If we order feature by importance using KBest using f_classif. We got the following top5 features:
  </p>
  <ul>
    <li>AgeID - 23258.7838511</li>
    <li>BloodpressureSystolicUpperID - 16178.6254018</li>
    <li>BloodpressureSystolicLowerID - 14223.2129934</li>
    <li>PulsefrequencyRegularID - 11020.7966351</li>
    <li>SaturationWithOxygenStatusID - 10664.3865356</li>
    <li>BreathingFrequencyID - 10321.2302924</li>
  </ul>
  <p>
    If we order feature by weight which random forest assigns to them we got the following top 5 features:
  </p>
  <ul>
    <li>WayOfArrivalID (0.146988)</li>
    <li>ProblemCauseID (0.109143)</li>
    <li>VisitCauseID (0.106607)</li>
    <li>AgeID (0.103987)</li>
    <li>ArrivalHour (0.099533)</li>
  </ul>
  <p>
    Random forest gives results, which take into account other features, so we would trust it more in this case.
  </p>
  <h2>ML Method Selection</h2>
  <p>
    We've chosen the following methods:
  </p>
  <ul>
    <li>Decision trees. They are know to work quite well for data with missing values which we have here</li>
    <li>Random forests. They are basically decision trees with lower variance</li>
    <li>Gradient boosting. They are another improvement on decision trees with lower variance</li>

    <li>Logistic Regression.</li>
    <li>Linear SVM (data set is very large and using kernelized SVM isn't good idea here)</li>
  </ul>

  <h2>ROC</h2>
  <p>
    Here's the ROC curve for models which we used:
  </p>
  <img src="images/roc.png" />
  <p>
    As we see, the worst method are decision trees which is unsurprising. The method has high variance and very
    susceptible to overfitting. Logistic regression and Linear SVM are substantially better and quite close to
    each other. Random forest and Gradient Boosting Classifier are the best two, with Gradient Boosting Classifier
    performing a little bit better than random forest.
  </p>
  <p>
    Overall the best method is Gradient Boosting Classifier. However, the method can be used with different threshold
    values, and we need to find a value which is the best in some sense. We will cover this in the next section.
  </p>
  <h2>Cost Matrix</h2>
  <p>
    This is were prediction models make contact with the real world. Putting things in numbers representing hard cash
    gets peoples attention. We started by collecting some cost data and then settled on case cost if a patient is
    admitted (In-patient) as our driving factor.
  </p>
  <p>
    One In-patient case cost is in average 68 000 SEK which is about 8 000 USD. Armed with this we predicted an outcome
    of each model using our test data and compared this with the actual outcome in a confusion matrix. This confusion
    matrix served as input to our cost function were we applied the following costs:
  </p>

  <h3>Should be sent home</h3>
  <p>
    True negatives ( When Actual Other is Predicted as Other )
  </p>
  <p>
    From both patient and hospital this is the best outcome. As a patient you were worried, came to the ER and 4h + a
    handful tax dollars (well actually about 400 USD) later you were sent home hopefully a little less anxious maybe
    with an ordination for some pain killers or a referral to an out-patient doctor.
  </p>
  <p>Cost estimate: 0</p>


  <h3>Needs hospitalization but is sent home</h3>
  <p>
    False negative ( When Actual In-patient is predicted as Other )
  </p>
  <p>
    For the patient this is the worst case. The ER failed to recognize that you need to be hospitalized to get well and
    after a night in pain you go back to the ER and finally they commit to to a hospital bed, the only thing is that now
    you feel worse, much worse than yesterday. The cost for this case were discussed to a length, we first set the cost
    to 0 arguing that from a pure bean counting hospital controllers perspective there is really no additional cost
    compared to a true positive other than maybe the extra ER visit, in the end we settled on a cost model where the
    cost for the false negative is a bit higher than for a true positive as most probably the patient will need extra
    care  due to their late admission to good care and medicine.
  </p>
  <p>
    Cost estimate: 85 000 SEK (10 000 USD)
  </p>

  <h3>Hospitalized but should have gone home</h3>
  <p>
    False positive: (When Actual Other is predicted as In-patient)
  </p>
  <p>
    This is not good for either party but it is not as severe as a False negative. We made the assumption that the
    hospital will discover that the patient is quite alright and then discharge them early.
  </p>
  <p>
    Cost estimate: 68000 * 25% = 17 000 SEK (2 000 USD )
  </p>

  <h3>In need of and gets a hospital bed</h3>
  <p>
    True positive: ( When Actual In-patient is predicted as In-patient )
  </p>
  <p>
    This is actually good for the patient but still incurs a cost for the hospital. It's hard to to go into the macro
    economic effects so we kept to the micro scenario (Hospital economics) and just assigned the average cost for an
    In-patient case.
  </p>
  <p>
    Cost estimate: 68 000 SEK (8 000 USD)
  </p>

  <h3>Cost Comparsion</h3>
  <img src="images/matrices.png" width="600px"/>
  <p>
    As you can see, cost-wise the best model is linear support vector machine. However, the model was used with the
    default threshold, so these results are inconclusive. We need a separate step to find the optimal threshold which
    can be explored in the future.
  </p>

  <h1>Conclusion</h1>
  <p>
    In this project, we extracted the data from the database, cleaned it up, performed exploratory data analaysis,
    then choosen machine learning methods, applied, and compared them.  We learned how to use git in collaborative
    setting with IPython notebooks, had experience of collaborating with the team over the internet in different time
    zones.
  </p>
  <p>
    We used accuracy and AUC as criteria for comparing the models. According to all the criteria, the
    best model is gradient boosting, which is an ensemble variation of a decision tree model.
  </p>
  <p>
    Cost matrix is the most realistic cost model among the ones we used, but we didn't have time to explore it
    completely. We used it to compare the models with the default threshold instead of finding the optimal threshold
    which could be improved with further work. According to default comparisons, LSVM is the best model, but we
    can change the threshold, and as a result the cost might change, so we can't trust these results
  </p>
  <p>
    The results of our predictions show, how important and nuanced the work of doctors and other medical personnel
    is. Despite the fact, that we have a large amount of quantitative and qualitative information about the patient,
    we were able to find a model which is only 4% better in accuracy than the baseline. Doctors, not only take into
    account the vitals, and other information which we have, but the medical history, their intuition, which are
    quite hard to process and learn from for machine today.
  </p>


  <h1>Links</h1>
  <ul>
    <li><a href="https://github.com/solomatov/cs109-project">Project Repository</a></li>
    <li><a href="https://www.youtube.com/watch?v=5_jx9xADA6k&feature=youtu.be">Project Video</a></li>
  </ul>
</div>

<script type="text/javascript">
  $(function() {
    $('#toc').tocify();
  });
</script>

</body>
</html>